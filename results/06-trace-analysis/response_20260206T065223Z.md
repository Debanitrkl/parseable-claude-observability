

# Distributed Trace Analysis

## Trace ID: `971a659583cb354ebf2a7babf50db2f8`

---

## Step 0: Deduplication

Before reconstructing the tree, I note that several spans appear multiple times in the flat array (once per event on that span ‚Äî e.g., "Enqueued", "Sent", "ResponseReceived"). After deduplication by `span_span_id`, we have **20 unique spans**:

| # | span_id | service | span_name | kind | duration_ns |
|---|---------|---------|-----------|------|-------------|
| 1 | `5ecdd5c42a31c2e5` | load-generator | user_add_to_cart | INTERNAL | 1,080,960,237 |
| 2 | `5cc7defd8ac01b04` | load-generator | GET | CLIENT | 6,005,672 |
| 3 | `88dd4ed0cfa16aaf` | load-generator | POST | CLIENT | 1,074,400,892 |
| 4 | `673bcdce936c36fd` | frontend-proxy | ingress (GET) | SERVER | 4,248,000 |
| 5 | `22048ba783f25d7f` | frontend-proxy | router frontend egress (GET) | CLIENT | 4,175,000 |
| 6 | `36dde1a91ce7e0ba` | frontend-proxy | ingress (POST) | SERVER | 1,069,754,000 |
| 7 | `66a43a0d5d10cc66` | frontend-proxy | router frontend egress (POST) | CLIENT | 1,069,682,000 |
| 8 | `40eedf31d5f77aa0` | frontend | GET /api/products/HQTGWGPNH4 | SERVER | 2,694,720 |
| 9 | `6317d5ec880c2a7d` | frontend | GET (next.js handleRequest) | SERVER | 2,559,550 |
| 10 | `dd16246c312a3ac8` | frontend | executing api route ‚Ä¶ /api/products/[productId] | INTERNAL | 2,226,089 |
| 11 | `c3f8f25fbccb589c` | frontend | grpc.oteldemo.ProductCatalogService/GetProduct | CLIENT | 2,076,177 |
| 12 | `634c1ace3f902dc0` | product-catalog | oteldemo.ProductCatalogService/GetProduct | SERVER | 154,881 |
| 13 | `e1554eb4fe796276` | frontend | POST /api/cart | SERVER | 1,069,494,306 |
| 14 | `1ac5140b5da23d72` | frontend | POST (next.js handleRequest) | SERVER | 1,069,375,205 |
| 15 | `450b340462c0a8f4` | frontend | executing api route ‚Ä¶ /api/cart | INTERNAL | 1,069,144,803 |
| 16 | `0ebfca528fb2a3c7` | frontend | grpc.oteldemo.CartService/AddItem | CLIENT | 1,066,789,345 |
| 17 | `06be3d91dbd37f1d` | frontend | grpc.oteldemo.CartService/GetCart | CLIENT | 2,156,688 |
| 18 | `940f2b8ead0ffb3d` | cart | POST /oteldemo.CartService/AddItem | SERVER | 382,800 |
| 19 | `52291a864040f3d6` | cart | POST /oteldemo.CartService/GetCart | SERVER | 1,327,300 |
| 20 | `20477715a96a4d58` | frontend | dns.lookup | CLIENT | 1,361,055 |
| 21 | `c4e8b74e72625cd3` | frontend | tcp.connect | INTERNAL | 778,763 |
| ‚Äî | `43f98c9155372669` | cart | HGET (redis, under AddItem) | CLIENT | 215,900 |
| ‚Äî | `9a0ee73806625703` | cart | HMSET (redis) | CLIENT | 68,300 |
| ‚Äî | `e575504609c9c07c` | cart | EXPIRE (redis) | CLIENT | 58,200 |
| ‚Äî | `cc90d8ba05ba9432` | cart | HGET (redis, under GetCart) | CLIENT | 80,000 |

---

## 1. Service Call Tree Reconstruction

```
[load-generator] user_add_to_cart (INTERNAL) ................ 1,080.96 ms   ‚Üê ROOT
‚îú‚îÄ‚îÄ [load-generator] GET http://.../api/products/HQTGWGPNH4 (CLIENT) .... 6.01 ms
‚îÇ   ‚îî‚îÄ‚îÄ [frontend-proxy] ingress GET (SERVER) ........................... 4.25 ms
‚îÇ       ‚îî‚îÄ‚îÄ [frontend-proxy] router frontend egress GET (CLIENT) ........ 4.18 ms
‚îÇ           ‚îî‚îÄ‚îÄ [frontend] GET /api/products/HQTGWGPNH4 (SERVER) ........ 2.69 ms
‚îÇ               ‚îî‚îÄ‚îÄ [frontend] GET next.js handleRequest (SERVER) ........ 2.56 ms
‚îÇ                   ‚îî‚îÄ‚îÄ [frontend] executing api route /api/products/[productId] (INTERNAL) .. 2.23 ms
‚îÇ                       ‚îî‚îÄ‚îÄ [frontend] grpc.ProductCatalogService/GetProduct (CLIENT) .. 2.08 ms
‚îÇ                           ‚îî‚îÄ‚îÄ [product-catalog] GetProduct (SERVER) .... 0.15 ms
‚îÇ
‚îî‚îÄ‚îÄ [load-generator] POST http://.../api/cart (CLIENT) .............. 1,074.40 ms
    ‚îî‚îÄ‚îÄ [frontend-proxy] ingress POST (SERVER) ...................... 1,069.75 ms
        ‚îî‚îÄ‚îÄ [frontend-proxy] router frontend egress POST (CLIENT) ... 1,069.68 ms
            ‚îî‚îÄ‚îÄ [frontend] POST /api/cart (SERVER) .................. 1,069.49 ms
                ‚îî‚îÄ‚îÄ [frontend] POST next.js handleRequest (SERVER) .. 1,069.38 ms
                    ‚îî‚îÄ‚îÄ [frontend] executing api route /api/cart (INTERNAL) .. 1,069.14 ms
                        ‚îú‚îÄ‚îÄ [frontend] grpc.CartService/AddItem (CLIENT) .. 1,066.79 ms
                        ‚îÇ   ‚îú‚îÄ‚îÄ [frontend] dns.lookup (CLIENT) ............. 1.36 ms
                        ‚îÇ   ‚îú‚îÄ‚îÄ [frontend] tcp.connect (INTERNAL) .......... 0.78 ms
                        ‚îÇ   ‚îî‚îÄ‚îÄ [cart] POST /CartService/AddItem (SERVER) .. 0.38 ms
                        ‚îÇ       ‚îú‚îÄ‚îÄ [cart] HGET redis (CLIENT) ............. 0.22 ms
                        ‚îÇ       ‚îú‚îÄ‚îÄ [cart] HMSET redis (CLIENT) ............ 0.07 ms
                        ‚îÇ       ‚îî‚îÄ‚îÄ [cart] EXPIRE redis (CLIENT) ........... 0.06 ms
                        ‚îÇ
                        ‚îî‚îÄ‚îÄ [frontend] grpc.CartService/GetCart (CLIENT) ... 2.16 ms
                            ‚îî‚îÄ‚îÄ [cart] POST /CartService/GetCart (SERVER) ... 1.33 ms
                                ‚îî‚îÄ‚îÄ [cart] HGET redis (CLIENT) .............. 0.08 ms
```

---

## 2. Critical Path Identification

The **overall trace duration** is **1,080.96 ms** (the root span `user_add_to_cart`). The critical path follows the longest chain through the POST /api/cart branch:

| Span | Service | Duration | Self-Time on Critical Path |
|------|---------|----------|---------------------------|
| `user_add_to_cart` | load-generator | 1,080.96 ms | 6.56 ms *(overhead between issuing GET, POST and collecting results)* |
| `POST` (HTTP client) | load-generator | 1,074.40 ms | 4.65 ms *(HTTP client overhead)* |
| `ingress` (POST) | frontend-proxy | 1,069.75 ms | 0.07 ms |
| `router frontend egress` (POST) | frontend-proxy | 1,069.68 ms | 0.19 ms |
| `POST /api/cart` (HTTP server) | frontend | 1,069.49 ms | 0.12 ms |
| `POST` (next.js handleRequest) | frontend | 1,069.38 ms | 0.23 ms |
| `executing api route /api/cart` | frontend | 1,069.14 ms | 0.20 ms |
| **`grpc.CartService/AddItem`** | **frontend** | **1,066.79 ms** | **~1,066.02 ms** ‚ö†Ô∏è |
| `POST /CartService/AddItem` | cart | 0.38 ms | 0.04 ms |
| Redis HGET/HMSET/EXPIRE | cart | ~0.34 ms total | ‚Äî (leaf spans) |

**The critical path is entirely dominated by the `grpc.CartService/AddItem` CLIENT span in the frontend service.**

---

## 3. Bottleneck Detection

### üî¥ Bottleneck: `grpc.oteldemo.CartService/AddItem` (span `0ebfca528fb2a3c7`)

| Metric | Value |
|--------|-------|
| **Total duration** | 1,066,789,345 ns (**1,066.79 ms**) |
| **Child spans' total duration** | dns.lookup (1.36 ms) + tcp.connect (0.78 ms) + cart AddItem server (0.38 ms) ‚âà **2.52 ms** |
| **Self-time** | **~1,064.27 ms (99.76% of its duration)** |

**Why this is the bottleneck:** This gRPC client span on the `frontend` service represents the time from when the frontend initiated the `AddItem` RPC to when it received the response. The actual server-side work in the `cart` service took only **0.38 ms**, including three Redis operations. Yet the client-side span lasted **1,066.79 ms**.

The ~1,064 ms of unaccounted self-time is consumed by:

1. **TCP connection establishment**: The span includes a `dns.lookup` (1.36 ms) and `tcp.connect` (0.78 ms) ‚Äî the frontend had to open a **new gRPC connection** to the cart service. However, these only account for ~2.1 ms.
2. **gRPC channel setup / HTTP/2 negotiation**: After the TCP connection is established, HTTP/2 connection preface, SETTINGS frames, and potentially TLS handshake (if applicable) must complete. This is the most likely cause of the ~1,064 ms gap.
3. **Possible cold-start or resource contention**: The cart service container may have been starting up, or the gRPC channel experienced a long connection queue / backoff.

**In summary**: The frontend spent over 1 second establishing a gRPC connection to the cart service before it could send the actual AddItem request (which itself completed in < 0.4 ms).

---

## 4. Anomalous Span Detection

### ‚ö†Ô∏è Anomaly 1: Extreme gRPC client-to-server time gap (AddItem)
- **Client span** (`frontend`): `grpc.CartService/AddItem` = **1,066.79 ms**
- **Server span** (`cart`): `POST /CartService/AddItem` = **0.38 ms**
- **Ratio**: 2,800:1 ‚Äî The client waited **2,800√ó longer** than the server took to process. This indicates a severe connection establishment delay, not a server-side issue.

### ‚ö†Ô∏è Anomaly 2: DNS lookup and TCP connect as children of gRPC call
- `dns.lookup` (1.36 ms) and `tcp.connect` (0.78 ms) appear as children of the `AddItem` gRPC call, meaning the frontend **did not have a pre-existing connection** to the cart service. In a healthy microservices environment, gRPC connections are typically pooled and reused. This suggests either a **first request** (cold connection pool) or a **connection eviction/reset**.

### ‚ö†Ô∏è Anomaly 3: Sequential GET then POST with large time gap
- The `GET /api/products/HQTGWGPNH4` completed in **6 ms** (at ~T+3ms to T+9ms relative to trace start).
- The `POST /api/cart` started almost immediately after but took **1,074 ms**.
- Both are children of the same root span. The GET path (through product-catalog) is performant; the POST path (through cart) is **178√ó slower** despite doing comparable work.

### ‚úÖ No error-status anomalies
- All spans have `span_status_code: 0` (UNSET/OK) and HTTP 200 responses. No retries detected (no duplicate server-side spans for the same operation).

---

## 5. Plain-English Narrative

> A load-generator test simulated a user adding product "HQTGWGPNH4" (The Comet Book, quantity 10) to their shopping cart. The request first fetched the product details from the product catalog, which completed quickly in about 6 milliseconds. However, when the system attempted to add the item to the cart, the operation took over **1 second** ‚Äî nearly the entire duration of the request ‚Äî even though the cart service itself processed the addition in under half a millisecond. The delay occurred because the frontend service had to establish a brand-new network connection (DNS resolution + TCP handshake + gRPC/HTTP2 negotiation) to the cart service, suggesting the connection pool was either cold or had been reset. No errors occurred during processing, so this is a **latency issue, not a functional failure**, and it would likely resolve on subsequent requests once the connection is established and pooled.