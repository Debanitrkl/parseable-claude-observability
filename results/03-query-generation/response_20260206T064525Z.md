

# Experiment 03: Query Generation Results

## Query 1: P99 Latency — Top 10 Slowest API Endpoints

**Key schema observations from the data:**
- `span_duration_ns` is actually `Float64`, not `BIGINT` as the prompt suggested
- `span_status_code` is `Float64`, not `VARCHAR`
- `"http.method"` and `span_name` together identify an endpoint

```sql
SELECT
    "service.name",
    "http.method",
    span_name,
    COUNT(*) AS request_count,
    ROUND(AVG(span_duration_ns) / 1e6, 2) AS avg_latency_ms,
    ROUND(APPROX_PERCENTILE_CONT(span_duration_ns, 0.99) / 1e6, 2) AS p99_latency_ms,
    ROUND(MAX(span_duration_ns) / 1e6, 2) AS max_latency_ms
FROM
    "astronomy-shop-traces"
WHERE
    p_timestamp >= NOW() - INTERVAL '1 hour'
    AND "http.method" IS NOT NULL
    AND span_duration_ns IS NOT NULL
GROUP BY
    "service.name",
    "http.method",
    span_name
HAVING
    COUNT(*) >= 5
ORDER BY
    p99_latency_ms DESC
LIMIT 10;
```

**Design decisions:**
- Grouped by `"service.name"` + `"http.method"` + `span_name` because the same span name (e.g., `ingress`) can appear across different services and HTTP methods — this gives the most actionable breakdown.
- Divided nanoseconds by `1e6` for millisecond readability.
- `HAVING COUNT(*) >= 5` filters out noise from single-occurrence spans that would dominate p99 rankings without being statistically meaningful.
- Used `ROUND(..., 2)` so the output is human-scannable.

---

## Query 2: PromQL Alert Rule — Error Rate > 5%

```yaml
groups:
  - name: checkout-service-alerts
    rules:
      - alert: HighErrorRate
        expr: |
          (
            sum(rate(http_server_request_duration_seconds_count{job="checkout-service", http_status_code=~"5.."}[5m]))
          /
            sum(rate(http_server_request_duration_seconds_count{job="checkout-service"}[5m]))
          ) > 0.05
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Checkout service error rate exceeds 5%"
          description: "The checkout-service is returning {{ $value | humanizePercentage }} server errors (HTTP 5xx) over the last 5 minutes, exceeding the 5% threshold."
```

**Design decisions:**
- Used `http_server_request_duration_seconds_count` (the counter of total requests) rather than `_bucket` because we need request counts partitioned by status code, not histogram quantiles.
- `http_status_code=~"5.."` matches 500, 502, 503, etc. via regex.
- `for: 2m` adds a stabilization window — the condition must hold for 2 minutes before firing, preventing single-scrape spikes from paging on-call.
- Both `sum()` wrappers collapse across all instances/pods so we get a service-level ratio, not per-pod fragmentation.
- `$value | humanizePercentage` in the description gives responders the actual error rate in the alert notification.

---

## Query 3: Parent Trace Correlation — High-Latency Spans to Root Spans

```sql
WITH slow_spans AS (
    SELECT
        span_trace_id,
        span_span_id,
        span_parent_span_id,
        "service.name" AS slow_service,
        span_name AS slow_span_name,
        span_duration_ns AS slow_duration_ns
    FROM
        "astronomy-shop-traces"
    WHERE
        p_timestamp >= NOW() - INTERVAL '1 hour'
        AND span_duration_ns > 1000000000
),
root_spans AS (
    SELECT
        span_trace_id,
        "service.name" AS root_service,
        span_name AS root_span_name,
        span_duration_ns AS root_duration_ns
    FROM
        "astronomy-shop-traces"
    WHERE
        p_timestamp >= NOW() - INTERVAL '1 hour'
        AND (span_parent_span_id = '' OR span_parent_span_id IS NULL)
)
SELECT
    s.span_trace_id,
    r.root_service,
    r.root_span_name,
    ROUND(r.root_duration_ns / 1e6, 2) AS root_duration_ms,
    s.slow_service,
    s.slow_span_name,
    ROUND(s.slow_duration_ns / 1e6, 2) AS slow_duration_ms,
    ROUND(s.slow_duration_ns / NULLIF(r.root_duration_ns, 0) * 100, 1) AS pct_of_trace
FROM
    slow_spans s
    INNER JOIN root_spans r ON s.span_trace_id = r.span_trace_id
ORDER BY
    s.slow_duration_ns DESC
LIMIT 20;
```

**Design decisions:**

- **CTEs for readability and scan efficiency:** Both CTEs filter on `p_timestamp >= NOW() - INTERVAL '1 hour'` independently. This is important — without the time filter on `root_spans`, the join would scan the entire table for root spans, even though we only care about recent traces.

- **Join on `span_trace_id`:** This correlates each slow span back to the root span of its trace. A trace can have exactly one root span (where `span_parent_span_id` is empty or NULL), so this is a many-to-one join.

- **`pct_of_trace` column:** This calculated field (`slow_duration / root_duration * 100`) immediately answers "how much of the total trace time did this slow span consume?" — which is the key diagnostic question. A span taking 2s in a 2.1s trace is the bottleneck; a 2s span in a 30s trace suggests the real problem is elsewhere.

- **`NULLIF(r.root_duration_ns, 0)`:** Guards against division by zero if a root span somehow has zero duration.

- **Why INNER JOIN, not LEFT JOIN:** We only want slow spans that we can actually attribute to a root span. Orphan spans (where the root wasn't captured in the time window) are noise for this analysis.